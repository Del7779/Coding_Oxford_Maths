
window_fraction = 0.01
max_points = 1000
trials = 1000
N=1000
    # Compute the size of the recording window.
 n = max(1, floor(Int, window_fraction * N))
    full_points = 2 * n
    num_window_points = min(full_points, max_points)
    sampling_interval = full_points / num_window_points  # e.g. if full_points=2000 and max_points=1000, sampling_interval=2

    # Define the range of p-values (edge densities) for the recording window.
    p_values = range((N / 2 - n) / N, stop=(N / 2 + n) / N, length=num_window_points)

    # Allocate arrays to store the largest cluster sizes and susceptibility values.
    s_max_values = zeros(Float64, trials, num_window_points)
    chi_values = zeros(Float64, trials, num_window_points)

    for t in 1:trials
        # Use the on‐the‐fly shuffled generator of edges.
        edges = [(i,j) for i in 1:N for j in (i+1):N]
        edges = shuffle(edges)

        uf = UnionFind(N)   # Your union–find data structure (assumed to be defined elsewhere)
        record_idx = 1      # index in the sampling window (1 .. num_window_points)
        sample_counter = 0  # counts how many edges in the window have been processed

        # Process edges sequentially.
        for (edge_count, (u, v)) in enumerate(edges)
            uf_union(uf, u, v)
            # Only record data if edge_count is within the recording window.
            if (edge_count > (N / 2 - n)) && (edge_count <= (N / 2 + n))
                sample_counter += 1
                # Check if it's time to record a sample.
                if sample_counter ≥ sampling_interval * record_idx
                    # Compute cluster sizes.
                    component_sizes = zeros(Int, N)
                    for i in 1:N
                        root = uf_find(uf, i)
                        component_sizes[root] += 1
                    end
                    component_sizes = component_sizes[component_sizes .> 0]
                    largest_cluster = maximum(component_sizes)
                    # Exclude the largest cluster to compute susceptibility.
                    reduced_cluster = component_sizes[component_sizes .!= largest_cluster]
                    chi = 0.0
                    if sum(reduced_cluster) != 0
                        unique_vals = unique(reduced_cluster)
                        counts_arr = [count(==(val), reduced_cluster) for val in unique_vals]
                        chi = sum(counts_arr .* (unique_vals .^ 2)) / sum(reduced_cluster)
                    end

                    chi_values[t, record_idx] = chi
                    s_max_values[t, record_idx] = largest_cluster
                    record_idx += 1
                    # Stop if we've collected all the desired samples.
                    if record_idx > num_window_points
                        break
                    end
                end
            elseif edge_count > (N / 2 + n)
                break
            end
        end
    end


using Plots

# Plot chi_values for different trials
plot1 = plot(legend=false)
for t in 1:100
    plot!(plot1,p_values, chi_values[t, :], label="Trial $t")
end
plot(p_values, vec(mean(chi_values,dims=1)), label="Mean", color=:black, lw=2)
display(plot1)

unique_vals = unique(component_sizes)
counts_arr = [count(==(val), component_sizes) for val in unique_vals]
plot(unique_vals, counts_arr, seriestype=:scatter, xscale=:log10,  
     yscale=:log10, label="Cluster Size Distribution", xlabel="Cluster Size", ylabel="Frequency", title="Cluster Size Distribution")

pyplot()
using Pkg
using StatsPlots
using Plots
 # Switch backend to PyPlot.

x = randn(1000)

histogram(x, bins=30, normalize=:pdf,
    label="Histogram", alpha=0.4,
    xlabel="x", ylabel="Density", title="Histogram and Density Curve")
plot(density(component_sizes), label="Density Curve", lw=2, color=:red)